# LLM Prompt Recovery: QLoRA Fine-Tuning Solution

[![Jupyter Book Badge](https://jupyterbook.org/badge.svg)](https://alex-yang-upenn.github.io/kaggle-prompt-recovery/intro.html) ![Deployment Badge](https://github.com/alex-yang-upenn/kaggle-prompt-recovery/actions/workflows/deploy.yml/badge.svg) ![License](https://img.shields.io/badge/License-Apache_2.0-blue.svg) ![kaggle-badge](https://img.shields.io/badge/Kaggle-035a7d?style=for-the-badge&logo=kaggle&logoColor=white) ![pytorch-badge](https://img.shields.io/badge/PyTorch-%23EE4C2C.svg?style=for-the-badge&logo=PyTorch&logoColor=white) ![keras-badge](https://img.shields.io/badge/Keras-%23D00000.svg?style=for-the-badge&logo=Keras&logoColor=white)

A complete solution for Kaggle's LLM Prompt Recovery Competition, featuring an in-depth guide to QLoRA Fine-Tuning.

Notebooks are hosted on [Github Pages](https://alex-yang-upenn.github.io/kaggle-prompt-recovery/intro.html) with Jupyter Book. They are also available for viewing, copying, and editing on Kaggle ([Part 1](https://www.kaggle.com/code/chuhuayang/prompt-recovery-pt-1-generate-training-data), [Part 2](https://www.kaggle.com/code/chuhuayang/prompt-recovery-pt-2-fine-tuning), [Part 3](https://www.kaggle.com/code/chuhuayang/prompt-recovery-pt-3-evaluation)), complete with training data, pre-trained LLMs and Adapters, and runtime environment with GPU/TPU access.