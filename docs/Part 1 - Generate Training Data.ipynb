{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49ed22a9",
   "metadata": {
    "papermill": {
     "duration": 0.004049,
     "end_time": "2024-05-25T04:23:13.483788",
     "exception": false,
     "start_time": "2024-05-25T04:23:13.479739",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Part 1: Generate Training Data üìù\n",
    "\n",
    "***Full notebook contents viewable on [Kaggle](https://www.kaggle.com/code/chuhuayang/prompt-recovery-pt-1-generate-training-data).***\n",
    "\n",
    "First, we need to generate some training data to tune the model with. Three components are necessary:\n",
    "1. Original Texts - For best results, this should be a large and diverse set of paragraph-length texts\n",
    "2. Rewrite Prompts\n",
    "3. Re-written Text - Since the competition targets Gemma-7B, we will feed that model the first two components and obtain the output\n",
    "\n",
    "For Original Texts, since this is a Kaggle-hosted competition, we can take advantage of high quality natural language datasets available for free on Kaggle. To achieve diversity, we will use samples from four different well-curated datasets:\n",
    "- [Wikipedia Movie Plots](https://www.kaggle.com/datasets/jrobischon/wikipedia-movie-plots): This dataset contains descriptions of thousands of movies from around the world. We can use the long-form plot summaries as Original Texts.\n",
    "- [Emotions](https://www.kaggle.com/datasets/nelgiriyewithana/emotions): This is a collection of English twitter messages, labeled with corresponding emotions. For our purposes, we can just use the tweets.\n",
    "- [Wikibooks](https://www.kaggle.com/datasets/dhruvildave/wikibooks-dataset): This is a massive dataset, containing the complete contents of the [Wikibooks](https://www.wikibooks.org/) archives. We can use the `abstract` column from the English language table.\n",
    "\n",
    "For Rewrite Prompts, there are a variety of ways to obtain them. We can write them fully manually, or write [Mad Libs](https://www.madlibs.com/)-style templates and write a simple program to fill in the blanks with different words. We can also prompt LLMs to generate them. These notebooks will use a Kaggle dataset containing a [collection of prompts](https://www.kaggle.com/datasets/chuhuayang/llm-prompt-recovery-competition) generated by myself and others using these methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67cc9f1c",
   "metadata": {
    "papermill": {
     "duration": 0.003515,
     "end_time": "2024-05-25T04:23:13.490758",
     "exception": false,
     "start_time": "2024-05-25T04:23:13.487243",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Loading in Data\n",
    "\n",
    "We will add the aforementioned datasets to our notebook, then read them in. We will perform two basic pre-processing steps to obtain more representative samples.\n",
    "- For entries from Wikipedia Movie Plots and Wikibooks, because they can be very long, we will slice the text into chunks of 256 words, or tokens. This also helps save resources when performing training and inference.\n",
    "- Then, we will filter out entries that are too short or that contain sequences or characters not typical of English texts.\n",
    "\n",
    "In these notebooks, as an example, we will randomly select only a small subset of the available data to fine-tune on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2f44352",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-25T04:23:13.498913Z",
     "iopub.status.busy": "2024-05-25T04:23:13.498564Z",
     "iopub.status.idle": "2024-05-25T04:24:36.304598Z",
     "shell.execute_reply": "2024-05-25T04:24:36.303558Z"
    },
    "papermill": {
     "duration": 82.815273,
     "end_time": "2024-05-25T04:24:36.309255",
     "exception": false,
     "start_time": "2024-05-25T04:23:13.493982",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_14/3661208475.py:1: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            original_text\n",
      "48979   i feel positive about my k at the end of the m...\n",
      "176784  i am feeling generous i can put up files fille...\n",
      "148564  i have a feeling the defense is going to come ...\n",
      "136809  i woke up in my bed alone for the last time fe...\n",
      "261174  i am actually liking sofia and i feel this is ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           original_text\n",
      "8524   the problem now which Playboy shows Bugs a fly...\n",
      "11031  Singing-and-dancing stage star Julie (Betty Gr...\n",
      "53471  school for admission where she meets George af...\n",
      "55971  Dollar (played by Pawan Kumar (of Lucia (2013 ...\n",
      "67650  When the Japan Coast Guard investigates an aba...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           original_text\n",
      "83536  Punjab was divided between the nations of Indi...\n",
      "62306  Methodology: A body of practices, procedures, ...\n",
      "16513  A computer-aided translation tool, developed b...\n",
      "70892  The cause of mental disorders is usually unkno...\n",
      "84067  This quest allows players access to a secret p...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sqlite3 as sql\n",
    "\n",
    "MAX_LENGTH = 256\n",
    "def create_slices(x):\n",
    "    return [' '.join(x[i:min(i+MAX_LENGTH, len(x))]) for i in range(0, len(x), MAX_LENGTH)]\n",
    "\n",
    "def filter(x):\n",
    "    filtered_out = (\"==\" in x) or (len(x) < 50) or (not x.isascii()) \n",
    "    return not filtered_out\n",
    "\n",
    "def preprocessing(x):\n",
    "    processed_df = x.str.split().apply(create_slices)\n",
    "    processed_df = processed_df.explode(ignore_index=True).dropna()\n",
    "    bool_df = processed_df.apply(filter)\n",
    "    return processed_df[bool_df]\n",
    "    \n",
    "\n",
    "series_1 = preprocessing(pd.read_csv(\"/kaggle/input/emotions/text.csv\")[\"text\"])\n",
    "series_1 = series_1.sample(n=2000, random_state=0)\n",
    "df_1 = series_1.to_frame(\"original_text\")\n",
    "print(df_1.head())\n",
    "\n",
    "series_2 = preprocessing(pd.read_csv(\"/kaggle/input/wikipedia-movie-plots/wiki_movie_plots_deduped.csv\")[\"Plot\"])\n",
    "series_2 = series_2.sample(n=1500, random_state=0)\n",
    "df_2 = series_2.to_frame(\"original_text\")\n",
    "print(df_2.head())\n",
    "\n",
    "conn = sql.connect(\"/kaggle/input/wikibooks-dataset/wikibooks.sqlite\")\n",
    "\n",
    "series_3 = preprocessing(pd.read_sql_query(\"SELECT abstract FROM en\", conn)[\"abstract\"])\n",
    "series_3 = series_3.sample(n=1500, random_state=0)\n",
    "df_3 = series_3.to_frame(\"original_text\")\n",
    "print(df_3.head())\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b0d5088",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-25T04:24:36.317927Z",
     "iopub.status.busy": "2024-05-25T04:24:36.317554Z",
     "iopub.status.idle": "2024-05-25T04:24:36.325457Z",
     "shell.execute_reply": "2024-05-25T04:24:36.324575Z"
    },
    "papermill": {
     "duration": 0.014646,
     "end_time": "2024-05-25T04:24:36.327098",
     "exception": false,
     "start_time": "2024-05-25T04:24:36.312452",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n"
     ]
    }
   ],
   "source": [
    "df = pd.concat([df_1, df_2, df_3], ignore_index=True)\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a8d0d1",
   "metadata": {
    "papermill": {
     "duration": 0.002939,
     "end_time": "2024-05-25T04:24:36.332985",
     "exception": false,
     "start_time": "2024-05-25T04:24:36.330046",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Assembling Prompts\n",
    "\n",
    "Now, we will add the dataset containing example rewrite prompts to the notebook. Each chunk of text will be randomly matched up with one of the rewrite prompts. Then, we will place both components into a simple template, to get more consistent results from Gemma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82118962",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-25T04:24:36.340672Z",
     "iopub.status.busy": "2024-05-25T04:24:36.340404Z",
     "iopub.status.idle": "2024-05-25T04:24:36.440316Z",
     "shell.execute_reply": "2024-05-25T04:24:36.439573Z"
    },
    "papermill": {
     "duration": 0.105789,
     "end_time": "2024-05-25T04:24:36.441983",
     "exception": false,
     "start_time": "2024-05-25T04:24:36.336194",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_text</th>\n",
       "      <th>rewrite_prompt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i kinda thought that they might be giving some...</td>\n",
       "      <td>Convey the same message as this text but throu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>im being honest ive been feeling quite bitchy ...</td>\n",
       "      <td>Rewrite this text in the style of a formal dip...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i am trying to feel calmer and more relaxed ev...</td>\n",
       "      <td>Imagine this as a conversation between quirky ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i was called and invited to have a talk about ...</td>\n",
       "      <td>Craft a version of the paragraph suitable for ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>off to do the same. After overcoming many chal...</td>\n",
       "      <td>Elevate this text by introducing a compelling ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       original_text  \\\n",
       "0  i kinda thought that they might be giving some...   \n",
       "1  im being honest ive been feeling quite bitchy ...   \n",
       "2  i am trying to feel calmer and more relaxed ev...   \n",
       "3  i was called and invited to have a talk about ...   \n",
       "4  off to do the same. After overcoming many chal...   \n",
       "\n",
       "                                      rewrite_prompt  \n",
       "0  Convey the same message as this text but throu...  \n",
       "1  Rewrite this text in the style of a formal dip...  \n",
       "2  Imagine this as a conversation between quirky ...  \n",
       "3  Craft a version of the paragraph suitable for ...  \n",
       "4  Elevate this text by introducing a compelling ...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "random.seed(0)\n",
    "\n",
    "df.insert(1, \"rewrite_prompt\", value=\"\")\n",
    "prompts = pd.read_csv(\"/kaggle/input/prompt-recovery-sample-rewrite-prompts/prompts.csv\").iloc[:,0]\n",
    "for i in df.index:\n",
    "    df.at[i, \"rewrite_prompt\"] = random.choice(prompts)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afacb3ac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-25T04:24:36.450250Z",
     "iopub.status.busy": "2024-05-25T04:24:36.449991Z",
     "iopub.status.idle": "2024-05-25T04:24:36.542946Z",
     "shell.execute_reply": "2024-05-25T04:24:36.541931Z"
    },
    "papermill": {
     "duration": 0.099446,
     "end_time": "2024-05-25T04:24:36.544840",
     "exception": false,
     "start_time": "2024-05-25T04:24:36.445394",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "template = \"\"\"Instruction:\n",
    "{rewrite_prompt}\n",
    "\n",
    "Original Text:\n",
    "{original_text}\n",
    "\n",
    "Response:\n",
    "\"\"\"\n",
    "\n",
    "df[\"prompt\"] = df.apply(lambda row: template.format(rewrite_prompt=row.rewrite_prompt, \n",
    "                                                             original_text=row.original_text), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4cc48fc",
   "metadata": {
    "papermill": {
     "duration": 0.003276,
     "end_time": "2024-05-25T04:24:36.551592",
     "exception": false,
     "start_time": "2024-05-25T04:24:36.548316",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Inference with Gemma\n",
    "\n",
    "We are ready to run inference with Gemma and obtain the rewritten texts. Since Kaggle gives access to TPU accelerators, this notebook will showcase how to use TPUs to achieve significant increases in compute power. We will be using Keras with Jax backend. Jax has support for [Model Parallelism](https://huggingface.co/docs/transformers/v4.15.0/en/parallelism) techniques, enabling sharding, or partitioning, of weight and embedding tensors across the 8 TPU cores and distributed computation. This effectively combines the 8 cores' compute power and memory capacities, significantly speeding up computation time for a single batch and allowing larger models to be fit into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50535f55",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-25T04:24:36.560364Z",
     "iopub.status.busy": "2024-05-25T04:24:36.559738Z",
     "iopub.status.idle": "2024-05-25T04:26:25.309325Z",
     "shell.execute_reply": "2024-05-25T04:26:25.308361Z"
    },
    "papermill": {
     "duration": 108.75656,
     "end_time": "2024-05-25T04:26:25.311519",
     "exception": false,
     "start_time": "2024-05-25T04:24:36.554959",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "keras-nlp 0.8.1 requires keras-core, which is not installed.\r\n",
      "tensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.0.5 which is incompatible.\r\n",
      "tensorflow 2.15.0 requires ml-dtypes~=0.2.0, but you have ml-dtypes 0.3.2 which is incompatible.\r\n",
      "tensorflow 2.15.0 requires tensorboard<2.16,>=2.15, but you have tensorboard 2.16.2 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "tensorflow-cpu 2.16.1 requires keras>=3.0.0, but you have keras 2.15.0 which is incompatible.\r\n",
      "tensorflow-cpu 2.16.1 requires ml-dtypes~=0.3.1, but you have ml-dtypes 0.2.0 which is incompatible.\r\n",
      "tensorflow-cpu 2.16.1 requires tensorboard<2.17,>=2.16, but you have tensorboard 2.15.2 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "tensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.3.3 which is incompatible.\r\n",
      "tensorflow-cpu 2.16.1 requires ml-dtypes~=0.3.1, but you have ml-dtypes 0.2.0 which is incompatible.\r\n",
      "tensorflow-cpu 2.16.1 requires tensorboard<2.17,>=2.16, but you have tensorboard 2.15.2 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install -q tensorflow-cpu\n",
    "!pip install -q -U keras-nlp tensorflow-hub\n",
    "!pip install -q -U keras>=3\n",
    "!pip install -q -U tensorflow-text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd5b9223",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-25T04:26:25.321338Z",
     "iopub.status.busy": "2024-05-25T04:26:25.321039Z",
     "iopub.status.idle": "2024-05-25T04:26:34.298390Z",
     "shell.execute_reply": "2024-05-25T04:26:34.297483Z"
    },
    "papermill": {
     "duration": 8.984589,
     "end_time": "2024-05-25T04:26:34.300190",
     "exception": false,
     "start_time": "2024-05-25T04:26:25.315601",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0525 04:26:30.349712299     121 oauth2_credentials.cc:238]            oauth_fetch: UNKNOWN:C-ares status is not ARES_SUCCESS qtype=A name=metadata.google.internal. is_balancer=0: Domain name not found {created_time:\"2024-05-25T04:26:30.349696576+00:00\", grpc_status:2}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0),\n",
       " TpuDevice(id=1, process_index=0, coords=(0,0,0), core_on_chip=1),\n",
       " TpuDevice(id=2, process_index=0, coords=(1,0,0), core_on_chip=0),\n",
       " TpuDevice(id=3, process_index=0, coords=(1,0,0), core_on_chip=1),\n",
       " TpuDevice(id=4, process_index=0, coords=(0,1,0), core_on_chip=0),\n",
       " TpuDevice(id=5, process_index=0, coords=(0,1,0), core_on_chip=1),\n",
       " TpuDevice(id=6, process_index=0, coords=(1,1,0), core_on_chip=0),\n",
       " TpuDevice(id=7, process_index=0, coords=(1,1,0), core_on_chip=1)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jax\n",
    "\n",
    "jax.devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c0cd15",
   "metadata": {
    "papermill": {
     "duration": 0.004184,
     "end_time": "2024-05-25T04:26:34.308758",
     "exception": false,
     "start_time": "2024-05-25T04:26:34.304574",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We define `device_mesh` and `layout_map` configurations that describe how Gemma's tensors should be sharded. Then, using the **`keras.distribution`** API, we pass these configurations to the Jax backend, which the performs the sharding when the model weights are loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59765c9b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-25T04:26:34.319288Z",
     "iopub.status.busy": "2024-05-25T04:26:34.318125Z",
     "iopub.status.idle": "2024-05-25T04:26:44.243729Z",
     "shell.execute_reply": "2024-05-25T04:26:44.242906Z"
    },
    "papermill": {
     "duration": 9.932757,
     "end_time": "2024-05-25T04:26:44.245421",
     "exception": false,
     "start_time": "2024-05-25T04:26:34.312664",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"jax\"\n",
    "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"0.9\" # Pre-allocate 90% of TPU memory to minimize memory fragmentation and allocation\n",
    "\n",
    "import keras\n",
    "import keras_nlp\n",
    "\n",
    "# Create a device mesh with (1, 8) shape so that the weights are sharded across all 8 TPU cores.\n",
    "device_mesh = keras.distribution.DeviceMesh(\n",
    "    (1, 8),\n",
    "    [\"batch\", \"model\"],\n",
    "    devices=keras.distribution.list_devices())\n",
    "\n",
    "# Create a layout map and define how each layer's weights should be sharded\n",
    "layout_map = keras.distribution.LayoutMap(device_mesh)\n",
    "model_dim = \"model\"\n",
    "# Weights that match 'token_embedding/embeddings' will be sharded on 8 TPUs\n",
    "layout_map[\"token_embedding/embeddings\"] = (None, model_dim)\n",
    "# Use a regex to match against the query, key and value matrices in the decoder attention layers\n",
    "layout_map[\"decoder_block.*attention.*(query|key|value).*kernel\"] = (None, model_dim, None)\n",
    "# Shards the attention output, feed-forward gating, and feed-forward linear layers of the decoder\n",
    "layout_map[\"decoder_block.*attention_output.*kernel\"] = (None, None, model_dim)\n",
    "layout_map[\"decoder_block.*ffw_gating.*kernel\"] = (model_dim, None)\n",
    "layout_map[\"decoder_block.*ffw_linear.*kernel\"] = (None, model_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c1dd038",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-25T04:26:44.255503Z",
     "iopub.status.busy": "2024-05-25T04:26:44.254981Z",
     "iopub.status.idle": "2024-05-25T04:29:32.962265Z",
     "shell.execute_reply": "2024-05-25T04:29:32.961175Z"
    },
    "papermill": {
     "duration": 168.714349,
     "end_time": "2024-05-25T04:29:32.964205",
     "exception": false,
     "start_time": "2024-05-25T04:26:44.249856",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Attaching 'metadata.json' from model 'keras/gemma/keras/gemma_instruct_7b_en/2' to your Kaggle notebook...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Attaching 'metadata.json' from model 'keras/gemma/keras/gemma_instruct_7b_en/2' to your Kaggle notebook...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Attaching 'task.json' from model 'keras/gemma/keras/gemma_instruct_7b_en/2' to your Kaggle notebook...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Attaching 'config.json' from model 'keras/gemma/keras/gemma_instruct_7b_en/2' to your Kaggle notebook...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Attaching 'metadata.json' from model 'keras/gemma/keras/gemma_instruct_7b_en/2' to your Kaggle notebook...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Attaching 'metadata.json' from model 'keras/gemma/keras/gemma_instruct_7b_en/2' to your Kaggle notebook...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Attaching 'config.json' from model 'keras/gemma/keras/gemma_instruct_7b_en/2' to your Kaggle notebook...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Attaching 'config.json' from model 'keras/gemma/keras/gemma_instruct_7b_en/2' to your Kaggle notebook...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Attaching 'model.weights.h5' from model 'keras/gemma/keras/gemma_instruct_7b_en/2' to your Kaggle notebook...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Attaching 'metadata.json' from model 'keras/gemma/keras/gemma_instruct_7b_en/2' to your Kaggle notebook...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Attaching 'metadata.json' from model 'keras/gemma/keras/gemma_instruct_7b_en/2' to your Kaggle notebook...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Attaching 'preprocessor.json' from model 'keras/gemma/keras/gemma_instruct_7b_en/2' to your Kaggle notebook...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Attaching 'tokenizer.json' from model 'keras/gemma/keras/gemma_instruct_7b_en/2' to your Kaggle notebook...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Attaching 'tokenizer.json' from model 'keras/gemma/keras/gemma_instruct_7b_en/2' to your Kaggle notebook...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Attaching 'assets/tokenizer/vocabulary.spm' from model 'keras/gemma/keras/gemma_instruct_7b_en/2' to your Kaggle notebook...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'keras_nlp.src.models.gemma.gemma_decoder_block.GemmaDecoderBlock'>\n",
      "decoder_block_1/pre_attention_norm/scale                    (3072,)           PartitionSpec(None,)\n",
      "decoder_block_1/attention/query/kernel                      (16, 3072, 256)   PartitionSpec(None, 'model', None)\n",
      "decoder_block_1/attention/key/kernel                        (16, 3072, 256)   PartitionSpec(None, 'model', None)\n",
      "decoder_block_1/attention/value/kernel                      (16, 3072, 256)   PartitionSpec(None, 'model', None)\n",
      "decoder_block_1/attention/attention_output/kernel           (16, 256, 3072)   PartitionSpec(None, None, 'model')\n",
      "decoder_block_1/pre_ffw_norm/scale                          (3072,)           PartitionSpec(None,)\n",
      "decoder_block_1/ffw_gating/kernel                           (3072, 24576)     PartitionSpec('model', None)\n",
      "decoder_block_1/ffw_gating_2/kernel                         (3072, 24576)     PartitionSpec('model', None)\n",
      "decoder_block_1/ffw_linear/kernel                           (24576, 3072)     PartitionSpec(None, 'model')\n"
     ]
    }
   ],
   "source": [
    "model_parallel = keras.distribution.ModelParallel(device_mesh, layout_map, batch_dim_name=\"batch\")\n",
    "\n",
    "keras.distribution.set_distribution(model_parallel)\n",
    "gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(\"gemma_instruct_7b_en\")\n",
    "\n",
    "# Print out information on one decoder block to verify weights were sharded correctly\n",
    "decoder_block_1 = gemma_lm.backbone.get_layer('decoder_block_1')\n",
    "print(type(decoder_block_1))\n",
    "for variable in decoder_block_1.weights:\n",
    "    print(f'{variable.path:<58}  {str(variable.shape):<16}  {str(variable.value.sharding.spec)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd4a765",
   "metadata": {
    "papermill": {
     "duration": 0.005021,
     "end_time": "2024-05-25T04:29:32.974361",
     "exception": false,
     "start_time": "2024-05-25T04:29:32.969340",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We will also cut Gemma's response off at 512 tokens, to prevent excessively long responses to certain prompts and conserve compute resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6490fca1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-25T04:29:32.985645Z",
     "iopub.status.busy": "2024-05-25T04:29:32.985356Z",
     "iopub.status.idle": "2024-05-25T10:26:27.969994Z",
     "shell.execute_reply": "2024-05-25T10:26:27.968726Z"
    },
    "papermill": {
     "duration": 21414.993875,
     "end_time": "2024-05-25T10:26:27.972916",
     "exception": false,
     "start_time": "2024-05-25T04:29:32.979041",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate(row):\n",
    "    output = gemma_lm.generate(row.prompt, max_length=512)\n",
    "    return output.replace(row.prompt, \"\") # Gemma's responses will repeat the entire user prompt. We remove this unnecessary repetition\n",
    "\n",
    "df[\"rewritten_text\"] = df.apply(generate, axis=1)\n",
    "df.to_csv(\"/kaggle/working/training_data.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "tpu1vmV38",
   "dataSources": [
    {
     "datasetId": 64890,
     "sourceId": 127736,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 1167113,
     "sourceId": 2730445,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4403839,
     "sourceId": 7563141,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5080045,
     "sourceId": 8510318,
     "sourceType": "datasetVersion"
    },
    {
     "modelInstanceId": 5391,
     "sourceId": 11373,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30666,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 21800.951452,
   "end_time": "2024-05-25T10:26:32.583562",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-05-25T04:23:11.632110",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
