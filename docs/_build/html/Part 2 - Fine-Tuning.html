
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Part 2: Fine-Tuning 🤗 🔢 &#8212; Kaggle LLM Prompt Recovery</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=3ee479438cf8b5e0d341" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=3ee479438cf8b5e0d341" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=3ee479438cf8b5e0d341" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=3ee479438cf8b5e0d341" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=384b581d" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=87e54e7c" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=3ee479438cf8b5e0d341" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=3ee479438cf8b5e0d341" />
  <script src="_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=3ee479438cf8b5e0d341"></script>

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=efea14e4"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script type="application/vnd.jupyter.widget-state+json">{"state": {"0364df3bc5294b6f8ffcf1d2cd615c6d": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "HTMLModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_4e6636295a7f4a04b882ff5503a43729", "placeholder": "\u200b", "style": "IPY_MODEL_385c2a18ce934fa7a45fa2569cd0d6b8", "value": "Loading checkpoint shards: 100%"}}, "0db11a39f95a4be8b755a8ad6a3a42b7": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "HBoxModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HBoxModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HBoxView", "box_style": "", "children": ["IPY_MODEL_e8c7beae3a3d4db9814dfe40b45b63a5", "IPY_MODEL_2c6cf255fc4048a8bf886b1ff3f85089", "IPY_MODEL_95b4922fa97c4f15923a49a82dec20d2"], "layout": "IPY_MODEL_d14532d4a44a4e9a8de75559b22715ee"}}, "16c15d7ba0774dd486c4a4e7b5a341bc": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "DescriptionStyleModel", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "17edfe54f00040cab23a17bb87809089": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "DescriptionStyleModel", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "1bc3c36e2758424c8106c755fd9925f8": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "DescriptionStyleModel", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "1f99800ef83346fbaa2dd811760771ed": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "2a706350dc974f0091211a7a6beb98a7": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "FloatProgressModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "FloatProgressModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "ProgressView", "bar_style": "success", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_c7fad07c8e064382b4e8ad21a4111601", "max": 4200, "min": 0, "orientation": "horizontal", "style": "IPY_MODEL_901157357949425ba62287a12ec93ccc", "value": 4200}}, "2c6cf255fc4048a8bf886b1ff3f85089": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "FloatProgressModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "FloatProgressModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "ProgressView", "bar_style": "success", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_587c0d9346414198875a9cf32fb9e0a2", "max": 200, "min": 0, "orientation": "horizontal", "style": "IPY_MODEL_7afa0e34d7f64a248cf4eb8a177f2384", "value": 200}}, "385c2a18ce934fa7a45fa2569cd0d6b8": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "DescriptionStyleModel", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "433103a0aee941dbb62687ea292d49c9": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "HBoxModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HBoxModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HBoxView", "box_style": "", "children": ["IPY_MODEL_49f6df7f21444c22a20371a5948ea63d", "IPY_MODEL_2a706350dc974f0091211a7a6beb98a7", "IPY_MODEL_aba1bd952be44a0eb875a020b96be3bc"], "layout": "IPY_MODEL_b2c442f58267413fa175146ee124afa2"}}, "498339bc6be1458bae98c291804fd152": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "ProgressStyleModel", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "ProgressStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "bar_color": null, "description_width": ""}}, "49f6df7f21444c22a20371a5948ea63d": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "HTMLModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_53830819d2054665ab20a9d7990e4470", "placeholder": "\u200b", "style": "IPY_MODEL_67c6a77b080a41ed8495a603be5c670b", "value": "Map: 100%"}}, "4e6636295a7f4a04b882ff5503a43729": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "53830819d2054665ab20a9d7990e4470": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "587c0d9346414198875a9cf32fb9e0a2": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "67c6a77b080a41ed8495a603be5c670b": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "DescriptionStyleModel", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "7afa0e34d7f64a248cf4eb8a177f2384": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "ProgressStyleModel", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "ProgressStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "bar_color": null, "description_width": ""}}, "7b69fed51d604f2594a27a842faee659": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "87839db44c2c43fdb76a3c7ea404e307": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "901157357949425ba62287a12ec93ccc": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "ProgressStyleModel", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "ProgressStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "bar_color": null, "description_width": ""}}, "95b4922fa97c4f15923a49a82dec20d2": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "HTMLModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_ad3b92d19d5b40b68252795fc659ceb3", "placeholder": "\u200b", "style": "IPY_MODEL_9ba50a8e86ec42439929be71ddaa8ce9", "value": " 200/200 [00:00&lt;00:00, 1553.73 examples/s]"}}, "9ba50a8e86ec42439929be71ddaa8ce9": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "DescriptionStyleModel", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "aba1bd952be44a0eb875a020b96be3bc": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "HTMLModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_d590a3d6e27e4d4785d62bc11b4e4aff", "placeholder": "\u200b", "style": "IPY_MODEL_1bc3c36e2758424c8106c755fd9925f8", "value": " 4200/4200 [00:02&lt;00:00, 1712.80 examples/s]"}}, "aca544b8703d447d87774a73e0f1dc75": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "FloatProgressModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "FloatProgressModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "ProgressView", "bar_style": "success", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_87839db44c2c43fdb76a3c7ea404e307", "max": 4, "min": 0, "orientation": "horizontal", "style": "IPY_MODEL_498339bc6be1458bae98c291804fd152", "value": 4}}, "ad3b92d19d5b40b68252795fc659ceb3": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "b2c442f58267413fa175146ee124afa2": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "c6b6833476c24b28afa108c0ed004d7b": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "HTMLModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_e808b04f97814022817072320e34d423", "placeholder": "\u200b", "style": "IPY_MODEL_16c15d7ba0774dd486c4a4e7b5a341bc", "value": " 4/4 [03:10&lt;00:00, 43.36s/it]"}}, "c7fad07c8e064382b4e8ad21a4111601": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "ceecdc82b9c148158fbd3e374777c28c": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "HBoxModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HBoxModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HBoxView", "box_style": "", "children": ["IPY_MODEL_0364df3bc5294b6f8ffcf1d2cd615c6d", "IPY_MODEL_aca544b8703d447d87774a73e0f1dc75", "IPY_MODEL_c6b6833476c24b28afa108c0ed004d7b"], "layout": "IPY_MODEL_1f99800ef83346fbaa2dd811760771ed"}}, "d14532d4a44a4e9a8de75559b22715ee": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "d590a3d6e27e4d4785d62bc11b4e4aff": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "e808b04f97814022817072320e34d423": {"model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "model_name": "LayoutModel", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "e8c7beae3a3d4db9814dfe40b45b63a5": {"model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "model_name": "HTMLModel", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_7b69fed51d604f2594a27a842faee659", "placeholder": "\u200b", "style": "IPY_MODEL_17edfe54f00040cab23a17bb87809089", "value": "Map: 100%"}}}, "version_major": 2, "version_minor": 0}</script>
    <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script crossorigin="anonymous" data-jupyter-widgets-cdn="https://cdn.jsdelivr.net/npm/" src="https://cdn.jsdelivr.net/npm/@jupyter-widgets/html-manager@1.0.6/dist/embed-amd.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'Part 2 - Fine-Tuning';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Part 3: Evaluation ✒️ 📑" href="Part%203%20-%20Evaluation.html" />
    <link rel="prev" title="Part 1: Generate Training Data 🗃️ 📝" href="Part%201%20-%20Generate%20Training%20Data.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/logo.png" class="logo__image only-light" alt="Kaggle LLM Prompt Recovery - Home"/>
    <script>document.write(`<img src="_static/logo.png" class="logo__image only-dark" alt="Kaggle LLM Prompt Recovery - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn navbar-btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    Prompt Recovery Competition
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="Part%201%20-%20Generate%20Training%20Data.html">Part 1: Generate Training Data 🗃️ 📝</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Part 2: Fine-Tuning 🤗 🔢</a></li>
<li class="toctree-l1"><a class="reference internal" href="Part%203%20-%20Evaluation.html">Part 3: Evaluation ✒️ 📑</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/alex-yang-upenn/kaggle-prompt-recovery" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/alex-yang-upenn/kaggle-prompt-recovery/issues/new?title=Issue%20on%20page%20%2FPart 2 - Fine-Tuning.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/Part 2 - Fine-Tuning.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Part 2: Fine-Tuning 🤗 🔢</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#set-up">Set-up</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#loading-the-model">Loading the Model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#loading-the-data">Loading the Data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#training-configurations">Training Configurations</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#training-and-saving">Training and Saving</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#results">Results</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="part-2-fine-tuning">
<h1>Part 2: Fine-Tuning 🤗 🔢<a class="headerlink" href="#part-2-fine-tuning" title="Link to this heading">#</a></h1>
<p><em><strong>Full notebook contents viewable on <a class="reference external" href="https://www.kaggle.com/code/chuhuayang/prompt-recovery-pt-2-fine-tuning">Kaggle</a>.</strong></em></p>
<p>Previously, we generated a set of training data. Now, we will use this data to fine-tune our model for better performance on the prompt recovery task. We will utilize the powerful <a class="reference external" href="https://huggingface.co/docs/transformers/index">HuggingFace Transformers API</a>  and its various integrations, which together provide a comprehensive collection of LLM training techniques and allows us to easily save, serialize, and deploy fine-tuned models.</p>
<section id="set-up">
<h2>Set-up<a class="headerlink" href="#set-up" title="Link to this heading">#</a></h2>
<p>Kaggle’s environments does not currently come pre-loaded with all the libraries from the Hugging Face ecosystem, so we will start by installing the necessary packages.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%%capture</span>

<span class="err">!</span><span class="n">pip</span> <span class="n">install</span> <span class="o">-</span><span class="n">q</span> <span class="o">-</span><span class="n">U</span> <span class="n">transformers</span>
<span class="err">!</span><span class="n">pip</span> <span class="n">install</span> <span class="o">-</span><span class="n">q</span> <span class="o">-</span><span class="n">U</span> <span class="n">accelerate</span>
<span class="err">!</span><span class="n">pip</span> <span class="n">install</span> <span class="o">-</span><span class="n">q</span> <span class="o">-</span><span class="n">U</span> <span class="n">bitsandbytes</span>
<span class="o">%</span><span class="n">pip</span> <span class="n">uninstall</span> <span class="o">-</span><span class="n">y</span> <span class="o">-</span><span class="n">q</span> <span class="n">datasets</span> 
<span class="o">%</span><span class="n">pip</span> <span class="n">install</span> <span class="o">-</span><span class="n">q</span> <span class="n">datasets</span><span class="o">==</span><span class="mf">2.16.0</span>
<span class="err">!</span><span class="n">pip</span> <span class="n">install</span> <span class="o">-</span><span class="n">q</span> <span class="o">-</span><span class="n">U</span> <span class="n">trl</span>
<span class="err">!</span><span class="n">pip</span> <span class="n">install</span> <span class="o">-</span><span class="n">q</span> <span class="o">-</span><span class="n">U</span> <span class="n">peft</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">warnings</span>

<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s2">&quot;ignore&quot;</span><span class="p">)</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>

<span class="kn">import</span> <span class="nn">transformers</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">BitsAndBytesConfig</span><span class="p">,</span> <span class="n">TrainingArguments</span>
<span class="kn">from</span> <span class="nn">datasets</span> <span class="kn">import</span> <span class="n">Dataset</span>
<span class="kn">from</span> <span class="nn">peft</span> <span class="kn">import</span> <span class="n">LoraConfig</span><span class="p">,</span> <span class="n">PeftConfig</span><span class="p">,</span> <span class="n">prepare_model_for_kbit_training</span><span class="p">,</span> <span class="n">get_peft_model</span>
<span class="kn">import</span> <span class="nn">bitsandbytes</span> <span class="k">as</span> <span class="nn">bnb</span>
<span class="kn">from</span> <span class="nn">trl</span> <span class="kn">import</span> <span class="n">SFTTrainer</span><span class="p">,</span> <span class="n">DataCollatorForCompletionOnlyLM</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2024-05-26 07:48:38.671776: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-05-26 07:48:38.671878: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-05-26 07:48:38.841896: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
</pre></div>
</div>
</div>
</div>
</section>
<section id="loading-the-model">
<h2>Loading the Model<a class="headerlink" href="#loading-the-model" title="Link to this heading">#</a></h2>
<p>We load the model and tokenizer using the Transformers library. <a class="reference external" href="https://huggingface.co/docs/bitsandbytes/main/en/index">bitsandbytes</a> is a library integrated with Transformers that allows quantization of LLMs in PyTorch to 8 or 4-bit. We create a bitsandbytes configuration and pass it to Transformers API when loading Gemma This significantly reduces the memory needed to run LLM inference. It also enables efficient LLM training techniques such as QLoRA, which we will be using.</p>
<p>Here is a simple run-through of the parameters. Further explanations can be found in these articles: <a class="reference external" href="https://huggingface.co/blog/hf-bitsandbytes-integration">Introduction</a>, <a class="reference external" href="https://huggingface.co/blog/4bit-transformers-bitsandbytes">4-bit Quantization</a></p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">load_in_4bit</span></code>: bitsandbytes supports FP4 precision, a further reduction of model size from 8-bit quantizations</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">bnb_4bit_use_double_quant</span></code>: This option saves even more memory by quantizing the scaling factors as well</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">bnb_4bit_quant_type</span></code>: We have a choice between nf4 and fp4 datatypes. The QLoRA paper recommends nf4.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">bnb_4bit_compute_dtype</span></code>: When the 4-bit weights are unpacked, they will be scaled to this datatype. We use <a class="reference external" href="https://cloud.google.com/blog/products/ai-machine-learning/bfloat16-the-secret-to-high-performance-on-cloud-tpus">bfloat16</a>, a datatype optimized for deep learning</p></li>
</ul>
<p>We set some additional configurations to optimize the training process</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">config.use_cache</span></code>: Caching is unnecessary during fine-tuning. Disabling this saves memory.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">config.pretraining_tp</span> <span class="pre">=</span> <span class="pre">1</span></code>: Disables tensor parallelism to avoid unexpected errors</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">gradient_checkpointing_enable()</span></code>: Applies the <a class="reference external" href="https://huggingface.co/docs/transformers/v4.18.0/en/performance#gradient-checkpointing">gradient checkpointing</a> strategy during the backward pass, reducing memory usage at the cost of longer compute time</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">bnb_config</span> <span class="o">=</span> <span class="n">BitsAndBytesConfig</span><span class="p">(</span>
    <span class="n">load_in_4bit</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">bnb_4bit_use_double_quant</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">bnb_4bit_quant_type</span><span class="o">=</span><span class="s2">&quot;nf4&quot;</span><span class="p">,</span>
    <span class="n">bnb_4bit_compute_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="s2">&quot;/kaggle/input/gemma/transformers/7b-it/3&quot;</span><span class="p">,</span>
    <span class="n">device_map</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">,</span>
    <span class="n">quantization_config</span><span class="o">=</span><span class="n">bnb_config</span><span class="p">,</span>
    <span class="n">torch_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span>
<span class="p">)</span>

<span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">use_cache</span> <span class="o">=</span> <span class="kc">False</span>
<span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">pretraining_tp</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">model</span><span class="o">.</span><span class="n">gradient_checkpointing_enable</span><span class="p">()</span>


<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;/kaggle/input/gemma/transformers/7b-it/3&quot;</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">padding_side</span> <span class="o">=</span> <span class="s2">&quot;right&quot;</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">add_eos_token</span> <span class="o">=</span> <span class="kc">True</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "ceecdc82b9c148158fbd3e374777c28c", "version_major": 2, "version_minor": 0}</script></div>
</div>
</section>
<section id="loading-the-data">
<h2>Loading the Data<a class="headerlink" href="#loading-the-data" title="Link to this heading">#</a></h2>
<p>To aid the LLM in learning, we will standardize inputs using a template with labels and an instruction set.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">TEMPLATE</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;### Instruction:</span>
<span class="s2">Below, the `Original Text` passage has been rewritten/transformed/improved into `Rewritten Text` by the `Gemma 7b-it` LLM with a certain prompt/instruction. Your task is to carefully analyze the differences between the &quot;Original Text&quot; and &quot;Rewritten Text&quot;, and try to infer the specific prompt or instruction that was likely given to the LLM to rewrite/transform/improve the text in this way.</span>

<span class="s2">### Original Text: </span>
<span class="si">{original_text}</span>

<span class="s2">### Rewritten Text:</span>
<span class="si">{rewritten_text}</span>

<span class="s2">### Response:</span>
<span class="si">{prompt}</span>
<span class="s2">&quot;&quot;&quot;</span>


<span class="k">def</span> <span class="nf">generate_prompt</span><span class="p">(</span><span class="n">row</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">TEMPLATE</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">original_text</span><span class="o">=</span><span class="n">row</span><span class="p">[</span><span class="s2">&quot;original_text&quot;</span><span class="p">],</span>
                           <span class="n">rewritten_text</span><span class="o">=</span><span class="n">row</span><span class="p">[</span><span class="s2">&quot;rewritten_text&quot;</span><span class="p">],</span>
                           <span class="n">prompt</span><span class="o">=</span><span class="n">row</span><span class="p">[</span><span class="s2">&quot;rewrite_prompt&quot;</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<p>Create train, test, and evaluation splits. Hugging Face uses its own <a class="reference external" href="https://huggingface.co/docs/datasets/main/en/index">Datasets</a> library. There is a simple function that converts Pandas Dataframes to Datasets.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">random</span>
<span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&quot;/kaggle/input/prompt-recovery-pt-1-generate-training/training_data.csv&quot;</span><span class="p">)</span>

<span class="n">df_train</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">4200</span><span class="p">]</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span><span class="o">.</span><span class="n">reset_index</span><span class="p">()</span>
<span class="n">df_eval</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="mi">4200</span><span class="p">:</span><span class="mi">4400</span><span class="p">]</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span><span class="o">.</span><span class="n">reset_index</span><span class="p">()</span>

<span class="n">df_train</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df_train</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">generate_prompt</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">df_eval</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df_eval</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">generate_prompt</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">df_train</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">df_eval</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">]))</span>

<span class="n">train_data</span> <span class="o">=</span> <span class="n">Dataset</span><span class="o">.</span><span class="n">from_pandas</span><span class="p">(</span><span class="n">df_train</span><span class="p">)</span>
<span class="n">eval_data</span> <span class="o">=</span> <span class="n">Dataset</span><span class="o">.</span><span class="n">from_pandas</span><span class="p">(</span><span class="n">df_eval</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>### Instruction:
Below, the `Original Text` passage has been rewritten/transformed/improved into `Rewritten Text` by the `Gemma 7b-it` LLM with a certain prompt/instruction. Your task is to carefully analyze the differences between the &quot;Original Text&quot; and &quot;Rewritten Text&quot;, and try to infer the specific prompt or instruction that was likely given to the LLM to rewrite/transform/improve the text in this way.

### Original Text: 
i have the feeling that ladislaus is not too keen on visitors at his place

### Rewritten Text:
Sure, here is the text rewritten through the eyes of an aspiring poet:

In the halls of whispers and secrets,
Ladislaus&#39;s abode, a sanctuary of dreams,
Yet a veil of caution hangs thick in the air,
For visitors, a burden he does not care.

The poet&#39;s heart, a canvas of longing,
Paints a picture of a heart that is torn,
Between the desire to share his soul and the fear of intrusion,
Ladislaus&#39;s stance, a reflection of his mood.

### Response:
Convey the same message as this text but through the eyes of an aspiring poet.

### Instruction:
Below, the `Original Text` passage has been rewritten/transformed/improved into `Rewritten Text` by the `Gemma 7b-it` LLM with a certain prompt/instruction. Your task is to carefully analyze the differences between the &quot;Original Text&quot; and &quot;Rewritten Text&quot;, and try to infer the specific prompt or instruction that was likely given to the LLM to rewrite/transform/improve the text in this way.

### Original Text: 
i am always ready to share my daily miseries with the world wide web and that is not about to change but having a day that left me feeling as defeated as yesterday has made today feel like a breeze

### Rewritten Text:
Sure, here is the text rewritten as a comedic script line:

&quot;Oh boy, I&#39;m ready to unload my daily dose of misery onto the intergalactic web, and you know what? That ain&#39;t changing. But let me tell you, having a day that made me feel as defeated as yesterday has made today feel like a breeze. I&#39;m talking cosmic cocktails and existential dread, baby!&quot;

Please note that I have added some comedic flair and exaggerated the language for comedic effect.

### Response:
Imagine the text as a line from a comedic script for an intergalactic sitcom.
</pre></div>
</div>
</div>
</div>
</section>
<section id="training-configurations">
<h2>Training Configurations<a class="headerlink" href="#training-configurations" title="Link to this heading">#</a></h2>
<p>The <a class="reference external" href="https://huggingface.co/docs/peft/index">PEFT</a> library, which we imported earlier, integrates seamlessly with other Hugging Face libraries like Trainer, bitsandbytes, and Accelerate.</p>
<p>We create a PEFT-enabled model. Here is a quick run-through of the LoRA hyperparameters. We will mostly follow the numbers used in the <a class="reference external" href="https://arxiv.org/abs/2305.14314">QLoRA paper’s</a> chatbot training:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">r</span></code>: The rank of the LoRA matrix being injected into the model. A larger number means more trainable parameters.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">lora_alpha</span></code>: Controls how much influence the LoRA weights have over model behavior. A larger number means more influence, and vice versa.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">lora_dropout</span></code>: Dropout is a common technique where randomly selected neurons are ignored during training, 0.1 means a 10% probability.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">target_modules</span></code>: Controls which modules LoRA will be applied to. We target all linear layers.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">prepare_model_for_kbit_training</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
<span class="n">peft_config</span> <span class="o">=</span> <span class="n">LoraConfig</span><span class="p">(</span>
    <span class="n">r</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span>
    <span class="n">lora_alpha</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>
    <span class="n">lora_dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="n">task_type</span><span class="o">=</span><span class="s2">&quot;CAUSAL_LM&quot;</span><span class="p">,</span>
    <span class="n">target_modules</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;q_proj&quot;</span><span class="p">,</span> <span class="s2">&quot;k_proj&quot;</span><span class="p">,</span> <span class="s2">&quot;v_proj&quot;</span><span class="p">,</span> <span class="s2">&quot;o_proj&quot;</span><span class="p">,</span>
                    <span class="s2">&quot;gate_proj&quot;</span><span class="p">,</span> <span class="s2">&quot;up_proj&quot;</span><span class="p">,</span> <span class="s2">&quot;down_proj&quot;</span><span class="p">]</span>
<span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">get_peft_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">peft_config</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">print_trainable_parameters</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>trainable params: 200,015,872 || all params: 8,737,696,768 || trainable%: 2.2891
</pre></div>
</div>
</div>
</div>
<p>Preparing for training on the 2 T4 GPUs set-up that Kaggle offers</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_count</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span> <span class="c1"># If more than 1 GPU</span>
    <span class="n">model</span><span class="o">.</span><span class="n">is_parallelizable</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="n">model</span><span class="o">.</span><span class="n">model_parallel</span> <span class="o">=</span> <span class="kc">True</span>

<span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">empty_cache</span><span class="p">()</span>

<span class="kn">import</span> <span class="nn">gc</span>
<span class="n">gc</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>51
</pre></div>
</div>
</div>
</div>
<p>Hugging Face’s <a class="reference external" href="https://huggingface.co/docs/trl/en/index">TRL library</a> offers a large array of tools for model training. We will be using the Supervised Fine-tuning Trainer. Here is a quick run-through of the arguments:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">save_steps</span></code> <code class="docutils literal notranslate"><span class="pre">save_total_limit</span></code> <code class="docutils literal notranslate"><span class="pre">load_best_model_at_end</span></code>: The Trainer saves checkpoints after every set number of steps. Our configuration will preserve the most recent checkpoint and the best checkpoint, and select the best checkpoint for use at the end of training.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">logging_steps</span></code> <code class="docutils literal notranslate"><span class="pre">report_to</span></code>: Logs training metrics after every set number of training steps. Integrated with tools such as <a class="reference external" href="https://docs.wandb.ai/guides">Weights &amp; Biases</a> and <a class="reference external" href="https://www.tensorflow.org/tensorboard">Tensorboard</a> for visualization and analysis. For simplicity, this notebook will not utilize any of these additional platforms.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">eval_strategy</span></code> <code class="docutils literal notranslate"><span class="pre">eval_steps</span></code>: Enables evaluation after every set number of training steps.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">per_device_train_batch_size</span></code> <code class="docutils literal notranslate"><span class="pre">gradient_accumulation_steps</span></code>: Training batchsize and number of batches to <a class="reference external" href="https://huggingface.co/docs/transformers/v4.18.0/en/performance#gradient-accumulation">accumulate gradients</a> for. We set the former to 1 to minimize memory usage and use the latter to mimic the effects of batched samples.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">per_device_eval_batch_size</span></code> <code class="docutils literal notranslate"><span class="pre">eval_accumulation_steps</span></code>: Controls batchsize and number of batches to evaluate before moving results to CPU. We set both to 1 to minimize memory usage.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">learning_rate</span></code> <code class="docutils literal notranslate"><span class="pre">lr_scheduler_type</span></code>: Sets the learning rate and learning rate schedule. Values are those used by QLoRA paper.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">weight_decay</span></code>: Adds a penalty term for larger weights to prevent overfitting</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">max_grad_norm</span></code>: Sets the threshold for gradient clipping. 0.3 is the value used by the QLoRA paper.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">fp16=True</span></code>: Enables training in half precision.</p></li>
</ul>
<p>We highlight two particularly important arguments</p>
<ul class="simple">
<li><p><strong><code class="docutils literal notranslate"><span class="pre">optim</span></code></strong>: We will use a special version of the Adam optimizer with weight decay, <a class="reference external" href="https://huggingface.co/docs/bitsandbytes/en/optimizers">paged_adamw_8bit</a>, offered by bitsandbytes. It is quantized to 8-bits, saving large amounts of memory and speeding up compute with no performance loss. Additionally, this optimizer leverages CUDA’s Unified Memory feature, utilizing CPU memory when the GPU runs out of memory.</p></li>
<li><p><strong><code class="docutils literal notranslate"><span class="pre">data_collator</span></code></strong>: Put simply, the <a class="reference external" href="https://huggingface.co/docs/transformers/en/main_classes/data_collator">Data Collator</a> class forms batches from a Dataset, and applies processing. The data collator used here, <code class="docutils literal notranslate"><span class="pre">DataCollatorForCompletionOnlyLM</span></code> ensures that all tokens before the user-defined response template do not contribute to the gradient. The model is trained on only the generated rewrite prompts. <code class="docutils literal notranslate"><span class="pre">packing=False</span></code> prevents any conflicts with this data collator.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">training_arguments</span> <span class="o">=</span> <span class="n">TrainingArguments</span><span class="p">(</span>
    <span class="n">output_dir</span><span class="o">=</span><span class="s2">&quot;./prompt-recovery-finetune&quot;</span><span class="p">,</span>
    <span class="n">save_steps</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span>
    <span class="n">save_total_limit</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">load_best_model_at_end</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">logging_steps</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span>
    <span class="n">report_to</span><span class="o">=</span><span class="s2">&quot;none&quot;</span><span class="p">,</span>
    <span class="n">eval_strategy</span><span class="o">=</span><span class="s1">&#39;steps&#39;</span><span class="p">,</span>
    <span class="n">eval_steps</span> <span class="o">=</span> <span class="mi">15</span><span class="p">,</span>
    <span class="n">num_train_epochs</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">gradient_checkpointing</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">per_device_train_batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">gradient_accumulation_steps</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
    <span class="n">per_device_eval_batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">eval_accumulation_steps</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">optim</span><span class="o">=</span><span class="s2">&quot;paged_adamw_8bit&quot;</span><span class="p">,</span>
    <span class="n">learning_rate</span><span class="o">=</span><span class="mf">2e-4</span><span class="p">,</span>
    <span class="n">lr_scheduler_type</span><span class="o">=</span><span class="s2">&quot;constant&quot;</span><span class="p">,</span>
    <span class="n">max_grad_norm</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span>
    <span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span>
    <span class="n">fp16</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>   
<span class="p">)</span>

<span class="n">response_template</span> <span class="o">=</span> <span class="s2">&quot;### Response:&quot;</span>
<span class="n">collator</span> <span class="o">=</span> <span class="n">DataCollatorForCompletionOnlyLM</span><span class="p">(</span><span class="n">response_template</span><span class="o">=</span><span class="n">response_template</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">)</span>

<span class="n">trainer</span> <span class="o">=</span> <span class="n">SFTTrainer</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
    <span class="n">data_collator</span><span class="o">=</span><span class="n">collator</span><span class="p">,</span>
    <span class="n">train_dataset</span><span class="o">=</span><span class="n">train_data</span><span class="p">,</span>
    <span class="n">eval_dataset</span><span class="o">=</span><span class="n">eval_data</span><span class="p">,</span>
    <span class="n">dataset_text_field</span><span class="o">=</span><span class="s2">&quot;text&quot;</span><span class="p">,</span>
    <span class="n">args</span><span class="o">=</span><span class="n">training_arguments</span><span class="p">,</span>
    <span class="n">packing</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "433103a0aee941dbb62687ea292d49c9", "version_major": 2, "version_minor": 0}</script><script type="application/vnd.jupyter.widget-view+json">{"model_id": "0db11a39f95a4be8b755a8ad6a3a42b7", "version_major": 2, "version_minor": 0}</script></div>
</div>
</section>
<section id="training-and-saving">
<h2>Training and Saving<a class="headerlink" href="#training-and-saving" title="Link to this heading">#</a></h2>
<p><code class="docutils literal notranslate"><span class="pre">trainer.train()</span></code> abstracts away the complicated training and evaluation loops, as well as Accelerate integrations.</p>
<p>Afterwards, instead of saving the entirety of the model’s weights, PEFT saves an Adapter, a folder consisting of only the new LoRA weights and configurations.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>If the notebook crashes with errors such as <code class="docutils literal notranslate"><span class="pre">RuntimeError:</span> <span class="pre">CUDA</span> <span class="pre">error:</span> <span class="pre">an</span> <span class="pre">illegal</span> <span class="pre">memory</span> <span class="pre">access</span> <span class="pre">was</span> <span class="pre">encountered</span></code>, we can easily resume training from the latest checkpoint with the argument <code class="docutils literal notranslate"><span class="pre">resume_from_checkpoint=True</span></code>. Ensure all other arguments are the same, and ensure <code class="docutils literal notranslate"><span class="pre">output_dir</span></code> is in the same state as before the training was interrupted. On Kaggle, since <code class="docutils literal notranslate"><span class="pre">/kaggle/working</span></code> resets between notebook runs, create a dataset containing the <code class="docutils literal notranslate"><span class="pre">outputs_dir</span></code> folder add it as an input for the next run.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="err">!</span><span class="n">cp</span> <span class="o">-</span><span class="n">r</span> <span class="o">/</span><span class="n">kaggle</span><span class="o">/</span><span class="nb">input</span><span class="o">/</span><span class="n">checkpoints</span><span class="o">/</span><span class="n">prompt_recovery_finetune</span> <span class="o">/</span><span class="n">kaggle</span><span class="o">/</span><span class="n">working</span><span class="o">/</span>

<span class="o">...</span>

<span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">resume_from_checkpoint</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>

<span class="n">trainer</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="s2">&quot;./adapter&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html">
    <div>
      
      <progress value='525' max='525' style='width:300px; height:20px; vertical-align: middle;'></progress>
      [525/525 6:08:23, Epoch 1/1]
    </div>
    <table border="1" class="dataframe">
  <thead>
 <tr style="text-align: left;">
      <th>Step</th>
      <th>Training Loss</th>
      <th>Validation Loss</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>15</td>
      <td>1.647700</td>
      <td>0.921029</td>
    </tr>
    <tr>
      <td>30</td>
      <td>0.810100</td>
      <td>0.693134</td>
    </tr>
    <tr>
      <td>45</td>
      <td>0.727000</td>
      <td>0.589362</td>
    </tr>
    <tr>
      <td>60</td>
      <td>0.555800</td>
      <td>0.558700</td>
    </tr>
    <tr>
      <td>75</td>
      <td>0.538900</td>
      <td>0.526677</td>
    </tr>
    <tr>
      <td>90</td>
      <td>0.560300</td>
      <td>0.439481</td>
    </tr>
    <tr>
      <td>105</td>
      <td>0.429500</td>
      <td>0.426885</td>
    </tr>
    <tr>
      <td>120</td>
      <td>0.399500</td>
      <td>0.377671</td>
    </tr>
    <tr>
      <td>135</td>
      <td>0.434000</td>
      <td>0.355361</td>
    </tr>
    <tr>
      <td>150</td>
      <td>0.387900</td>
      <td>0.336657</td>
    </tr>
    <tr>
      <td>165</td>
      <td>0.355000</td>
      <td>0.330116</td>
    </tr>
    <tr>
      <td>180</td>
      <td>0.424100</td>
      <td>0.304241</td>
    </tr>
    <tr>
      <td>195</td>
      <td>0.387400</td>
      <td>0.275571</td>
    </tr>
    <tr>
      <td>210</td>
      <td>0.282200</td>
      <td>0.281969</td>
    </tr>
    <tr>
      <td>225</td>
      <td>0.241100</td>
      <td>0.253618</td>
    </tr>
    <tr>
      <td>240</td>
      <td>0.341800</td>
      <td>0.259762</td>
    </tr>
    <tr>
      <td>255</td>
      <td>0.203700</td>
      <td>0.274455</td>
    </tr>
    <tr>
      <td>270</td>
      <td>0.289000</td>
      <td>0.244083</td>
    </tr>
    <tr>
      <td>285</td>
      <td>0.237600</td>
      <td>0.230947</td>
    </tr>
    <tr>
      <td>300</td>
      <td>0.273300</td>
      <td>0.239970</td>
    </tr>
    <tr>
      <td>315</td>
      <td>0.190100</td>
      <td>0.196642</td>
    </tr>
    <tr>
      <td>330</td>
      <td>0.244500</td>
      <td>0.227365</td>
    </tr>
    <tr>
      <td>345</td>
      <td>0.245700</td>
      <td>0.220264</td>
    </tr>
    <tr>
      <td>360</td>
      <td>0.196700</td>
      <td>0.212811</td>
    </tr>
    <tr>
      <td>375</td>
      <td>0.188000</td>
      <td>0.213365</td>
    </tr>
    <tr>
      <td>390</td>
      <td>0.233300</td>
      <td>0.196246</td>
    </tr>
    <tr>
      <td>405</td>
      <td>0.211900</td>
      <td>0.189716</td>
    </tr>
    <tr>
      <td>420</td>
      <td>0.196400</td>
      <td>0.209950</td>
    </tr>
    <tr>
      <td>435</td>
      <td>0.184300</td>
      <td>0.190855</td>
    </tr>
    <tr>
      <td>450</td>
      <td>0.161100</td>
      <td>0.176299</td>
    </tr>
    <tr>
      <td>465</td>
      <td>0.228200</td>
      <td>0.173038</td>
    </tr>
    <tr>
      <td>480</td>
      <td>0.152800</td>
      <td>0.161604</td>
    </tr>
    <tr>
      <td>495</td>
      <td>0.144700</td>
      <td>0.146135</td>
    </tr>
    <tr>
      <td>510</td>
      <td>0.118600</td>
      <td>0.158499</td>
    </tr>
    <tr>
      <td>525</td>
      <td>0.196500</td>
      <td>0.161419</td>
    </tr>
  </tbody>
</table><p></div></div>
</div>
</section>
<section id="results">
<h2>Results<a class="headerlink" href="#results" title="Link to this heading">#</a></h2>
<p>Adapters are a lightweight and flexible design. To load and run a trained QLoRA model, first load the base model like normal, using the same configurations. Then, use the PeftModel.from_pretrained() method, passing in the base model and the directory of the Adapter.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">base_model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="s2">&quot;/kaggle/input/gemma/transformers/7b-it/3&quot;</span><span class="p">,</span>
    <span class="n">device_map</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">,</span>
    <span class="n">quantization_config</span><span class="o">=</span><span class="n">bnb_config</span><span class="p">,</span>
    <span class="n">torch_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span>
<span class="p">)</span>
<span class="n">base_model</span><span class="o">.</span><span class="n">gradient_checkpointing_enable</span><span class="p">()</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>

<span class="o">...</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">PeftModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">base_model</span><span class="p">,</span> <span class="s2">&quot;/kaggle/input/</span><span class="si">{dataset_name}</span><span class="s2">/adapter&quot;</span><span class="p">,</span> <span class="n">adapter_name</span><span class="o">=</span><span class="s2">&quot;adapter_0&quot;</span><span class="p">,</span> <span class="n">is_trainable</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">enable_input_require_grads</span><span class="p">()</span>
</pre></div>
</div>
<p>This PeftModel can be used just like any normal model. We can perform inference using the <code class="docutils literal notranslate"><span class="pre">generate()</span></code> method. We can further finetune the Adapter by passing it into to a <code class="docutils literal notranslate"><span class="pre">SFTTrainer</span></code> set-up, just like above. This continues updating the LoRA weights of the Adapter.</p>
<p>PeftModels can even juggle multiple Adapters. After the PeftModel is instantiated, add more adapters with the <code class="docutils literal notranslate"><span class="pre">load_adapters()</span></code> method.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">load_adapter</span><span class="p">(</span><span class="s2">&quot;/kaggle/input/</span><span class="si">{dataset_name}</span><span class="s2">/</span><span class="si">{additional_adapter_1}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">adapter_name</span><span class="o">=</span><span class="s2">&quot;adapter_1&quot;</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">load_adapter</span><span class="p">(</span><span class="s2">&quot;/kaggle/input/</span><span class="si">{dataset_name}</span><span class="s2">/</span><span class="si">{additional_adapter_2}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">adapter_name</span><span class="o">=</span><span class="s2">&quot;adapter_2&quot;</span><span class="p">)</span>
<span class="o">...</span>
</pre></div>
</div>
<p>Only one Adapter can be active at a time. The active Adapter is used when performing inference and further finetuning. We can set any of the loaded Adapters as the active Adapter. We can also disable all Adapters to return to the base model</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">set_adapter</span><span class="p">(</span><span class="s2">&quot;adapter_1&quot;</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">disable_adapter</span><span class="p">()</span>
</pre></div>
</div>
<p>Finally, PEFT offers an algorithm for <a class="reference external" href="https://huggingface.co/docs/peft/en/developer_guides/model_merging">merging Adapters</a>, combining the abilities of separate Adapters into one. An introduction to the algorithm can be found <a class="reference external" href="https://huggingface.co/blog/peft_merging">here</a>.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">adapters</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;adapter_0&quot;</span><span class="p">,</span> <span class="s2">&quot;adapter_1&quot;</span><span class="p">,</span> <span class="s2">&quot;adapter_2&quot;</span><span class="p">]</span>
<span class="n">weights</span><span class="o">=</span><span class="p">[</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">]</span>
<span class="n">finetuned_model</span><span class="o">.</span><span class="n">add_weighted_adapter</span><span class="p">(</span><span class="n">adapters</span><span class="o">=</span><span class="n">adapters</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="n">weights</span><span class="p">,</span> <span class="n">adapter_name</span><span class="o">=</span><span class="s2">&quot;merge&quot;</span><span class="p">,</span> <span class="n">combination_type</span><span class="o">=</span><span class="s2">&quot;ties&quot;</span><span class="p">,</span> <span class="n">density</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
<span class="n">finetuned_model</span><span class="o">.</span><span class="n">set_adapter</span><span class="p">(</span><span class="s2">&quot;merge&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="Part%201%20-%20Generate%20Training%20Data.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Part 1: Generate Training Data 🗃️ 📝</p>
      </div>
    </a>
    <a class="right-next"
       href="Part%203%20-%20Evaluation.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Part 3: Evaluation ✒️ 📑</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#set-up">Set-up</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#loading-the-model">Loading the Model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#loading-the-data">Loading the Data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#training-configurations">Training Configurations</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#training-and-saving">Training and Saving</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#results">Results</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Chuhua Yang (Alex)
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=3ee479438cf8b5e0d341"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=3ee479438cf8b5e0d341"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>